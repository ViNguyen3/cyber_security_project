# -*- coding: utf-8 -*-
"""UNSW_NB15_FeatureSelect&Baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oPl_Jgvlla8aA3YO7cuoj6umdF7DBSbX
"""

# Basic tools
import pandas as pd
import numpy as np

# Preprocessing
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split

# Feature selection
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns



from google.colab import files
uploaded = files.upload()

# Example file names (adjust if yours differ)
train_path = 'UNSW_NB15_training-set.csv'
test_path  = 'UNSW_NB15_testing-set.csv'

df_train = pd.read_csv(train_path)
df_test  = pd.read_csv(test_path)

print("Training set shape:", df_train.shape)
print("Testing set shape:", df_test.shape)
df_train.head()

df_train['is_train'] = 1
df_test['is_train'] = 0

df_all = pd.concat([df_train, df_test], ignore_index=True)

drop_cols = ['id', 'srcip', 'sport', 'dstip', 'dsport']
df_all = df_all.drop(columns=drop_cols, errors='ignore')

# Binary label (normal=0, attack=1)
df_all['label'] = df_all['label'].apply(lambda x: 1 if x == 1 else 0)

# Drop attack category (optional)
if 'attack_cat' in df_all.columns:
    df_all = df_all.drop(columns=['attack_cat'])

# Encode categorical features
cat_cols = ['proto', 'service', 'state']
le = LabelEncoder()
for col in cat_cols:
    if col in df_all.columns:
        df_all[col] = le.fit_transform(df_all[col])

scaler = MinMaxScaler()
feature_cols = [col for col in df_all.columns if col not in ['label', 'is_train']]

df_all[feature_cols] = scaler.fit_transform(df_all[feature_cols])

train_df = df_all[df_all['is_train'] == 1].drop(columns=['is_train'])
test_df  = df_all[df_all['is_train'] == 0].drop(columns=['is_train'])

X_train = train_df.drop(columns=['label'])
y_train = train_df['label']

X_test = test_df.drop(columns=['label'])
y_test = test_df['label']

print("Final training shape:", X_train.shape)
print("Final testing shape:", X_test.shape)

rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

top_n = 20
top_features = X_train.columns[indices][:top_n]

print(f"Top {top_n} important features:")
print(top_features.tolist())

# Visualization
plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices][:top_n], y=X_train.columns[indices][:top_n], orient='h')
plt.title("Top Feature Importances (Random Forest)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()

X_train_selected = X_train[top_features]
X_test_selected  = X_test[top_features]

print("Train shape after feature selection:", X_train_selected.shape)
print("Test shape after feature selection:", X_test_selected.shape)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from xgboost import XGBClassifier

# Random Forest baseline
rf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=20, n_jobs=-1)
rf.fit(X_train_selected, y_train)

# Predictions
y_pred_rf = rf.predict(X_test_selected)

# Metrics
acc_rf = accuracy_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf)
rec_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print("ðŸŒ² Random Forest Results")
print("------------------------")
print(f"Accuracy:  {acc_rf:.4f}")
print(f"Precision: {prec_rf:.4f}")
print(f"Recall:    {rec_rf:.4f}")
print(f"F1 Score:  {f1_rf:.4f}")

# Confusion matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(5,4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal','Attack'], yticklabels=['Normal','Attack'])
plt.title("Random Forest Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# XGBoost baseline (lightweight parameters for real-time IDS)
xgb = XGBClassifier(
    n_estimators=200,
    max_depth=8,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric='logloss',
    use_label_encoder=False,
    n_jobs=-1,
    random_state=42
)
xgb.fit(X_train_selected, y_train)

# Predictions
y_pred_xgb = xgb.predict(X_test_selected)

# Metrics
acc_xgb = accuracy_score(y_test, y_pred_xgb)
prec_xgb = precision_score(y_test, y_pred_xgb)
rec_xgb = recall_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb)

print("âš¡ XGBoost Results")
print("------------------")
print(f"Accuracy:  {acc_xgb:.4f}")
print(f"Precision: {prec_xgb:.4f}")
print(f"Recall:    {rec_xgb:.4f}")
print(f"F1 Score:  {f1_xgb:.4f}")

# Confusion matrix
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(5,4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', xticklabels=['Normal','Attack'], yticklabels=['Normal','Attack'])
plt.title("XGBoost Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

results = pd.DataFrame({
    'Model': ['Random Forest', 'XGBoost'],
    'Accuracy': [acc_rf, acc_xgb],
    'Precision': [prec_rf, prec_xgb],
    'Recall': [rec_rf, rec_xgb],
    'F1-Score': [f1_rf, f1_xgb]
})

print("\nðŸ“Š Model Comparison Summary")
display(results.style.background_gradient(cmap='Blues', axis=0))

print("\nDetailed Classification Report (XGBoost)")
print(classification_report(y_test, y_pred_xgb, target_names=['Normal','Attack']))